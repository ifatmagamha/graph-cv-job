# Graph-based CV-Job Mining Project

Ce projet vise √† mod√©liser et analyser les interactions entre CV et Offres d'Emploi (Jobs) sous forme de graphe biparti. L'objectif final est d'am√©liorer le matching via des techniques de link prediction et de node classification enrichies par des LLM.

## Architecture Globale

Le pipeline suit une approche hybride combinant :
1. **S√©mantique** : Embeddings LLM (sentence-transformers) pour capturer le sens profond des textes.
2. **Structurel** : M√©triques de graphes (Centralit√©, Communaut√©s) pour capturer les relations topologiques.

## Objectifs Business
Ce projet vise √† r√©volutionner le recrutement gr√¢ce √† la **Th√©orie des Graphes** et aux **LLM**. Les objectifs principaux sont :
1.  **Automatisation du Matching** : Pr√©dire la pertinence d'un candidat pour un poste avec une pr√©cision sup√©rieure aux mots-cl√©s classiques.
2.  **Profiling Avanc√©** : Classifier les profils (Junior/Senior, Sp√©cialiste/Polyvalent) via leur positionnement dans l'√©cosyst√®me de comp√©tences.
3.  **Identification des Lacunes** : D√©tecter les liens manquants (opportunit√©s de carri√®re) qui ne sont pas visibles via une simple recherche s√©mantique.

## Mod√©lisation
-   **Graphe Biparti CV-Job** : Impl√©ment√© via NetworkX avec gestion des attributs sp√©cifiques √† chaque type de n≈ìud.
-   **Pr√©diction de Liens Hybride** : Fusion s√©mantique (Embeddings Gemini/SBERT) et structurelle (Common Neighbors, Preferential Attachment).
-   **Analyse de Communaut√©s** :
    -   *Global* : Identification de clusters de m√©tiers via la projection CV-CV.
    -   *Interne* : Mod√©lisation des graphes de comp√©tences (`src/graph/internal_skill_graph.py`) pour capturer la coh√©rence des profils.
-   **Classification** : Utilisation des features combin√©es pour cat√©goriser les n≈ìuds.

## Stack Technique
-   **Graph Engine** : NetworkX, PyVis (Visualisation).
-   **Intelligence Artificielle** : Google Gemini API (Link Prediction Zero-Shot), Sentence-Transformers (Embeddings).
-   **Machine Learning** : Scikit-learn (Fusion de features, Classification), FAISS (Recherche vectorielle rapide).
-   **Tracking** : MLflow pour le suivi des exp√©riences de Link Prediction.

## Choix des M√©triques et R√©sultats
Nous avons s√©lectionn√© des m√©triques sp√©cifiques √† deux probl√©matiques diff√©rentes :

### 1. Pr√©diction de Liens (Recommandation)
-   **Hits@10** (54.17%) : Capacit√© du syst√®me √† placer le bon job dans le top 10 des recommandations.
-   **MRR (Mean Reciprocal Rank)** : Important pour favoriser les syst√®mes qui placent les meilleures correspondances en haut de liste.
-   **ROC-AUC** (86.45%) : Mesure la capacit√© du mod√®le de fusion √† distinguer un vrai match d'un mauvais match.

### 2. Classification de N≈ìuds (Profiling)
-   **Accuracy** (95.00%) : Performance globale de la classification des domaines d'activit√©.
-   **Macro-F1 Score** : Utilis√© pour assurer une performance robuste m√™me sur les domaines moins repr√©sent√©s (√©quilibrage des classes).

## Analyse des Co√ªts
L'utilisation de LLM (Gemini) apporte une pr√©cision fine mais implique des co√ªts :
-   **Traitement Batch** : Pr√©f√©rable d'utiliser des embeddings locaux (SBERT) pour la recherche √† large √©chelle.
-   **Raffinement** : Le LLM est utilis√© uniquement sur les paires candidates √† haut score s√©mantique pour minimiser les appels API et la latence.

## D√©fis & Perspectives
### D√©fis rencontr√©s
-   **Sparsit√© des donn√©es** : Le graphe initial contient peu d'ar√™tes par rapport au nombre total de paires possibles.
-   **Complexit√© Bipartite** : Les algorithmes classiques de link prediction (ex: Adamic-Adar) ont d√ª √™tre adapt√©s pour la structure bipartite (chemins de longueur 3).

### Perspectives
-   **Graph Neural Networks (GNN)** : Passer de la r√©gression logistique √† un mod√®le type GraphSAGE pour un apprentissage de repr√©sentations plus profond.
-   **Enrichissement Temporel** : Int√©grer l'√©volution des carri√®res (temps pass√© sur un poste) comme poids dans le graphe.

## üìÇ Structure du Projet
-   `src/data` : Pr√©processing Parquet et extraction de skills.
-   `src/graph` : Construction, Communaut√©s, Graphes internes et Enrichissement.
-   `src/embeddings` : Embeddings multilingues et indexation FAISS.
-   `src/link_prediction` : Fusion supervis√©e et scoring LLM.
-   `src/classification` : √âtudes comparatives et profiling.
-   `main.py` : Orchestrateur du pipeline complet.

## Comment reproduire
1. Installer les d√©pendances : `pip install -r requirements.txt`
2. Lancer le pipeline complet : `python main.py`
3. Lancer le dashboard interactif : `streamlit run app.py`

## Robustesse & Tests
Pour v√©rifier l'int√©grit√© du syst√®me :
`python -m pytest src/tests/test_pipeline.py`